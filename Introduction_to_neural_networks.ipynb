{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are a class of machine learning algorithms inspired by the structure and functioning of the human brain. They are a powerful tool for solving complex problems and making predictions based on data. Neural networks consist of interconnected artificial neurons, also known as nodes or units, organized into layers.\n",
    "\n",
    "Here's a basic explanation of how neural networks work:\n",
    "\n",
    "1. **Input Layer**: The input layer is the first layer of the neural network, and it receives the raw data or features from the outside world. Each input node corresponds to a specific feature or attribute of the data.\n",
    "\n",
    "2. **Hidden Layers**: Between the input layer and the output layer, there can be one or more hidden layers. These layers are responsible for processing the data and extracting relevant patterns and features. The term \"hidden\" comes from the fact that their activations are not directly observable as inputs or outputs.\n",
    "\n",
    "3. **Output Layer**: The output layer is the final layer of the neural network. It produces the network's predictions or outputs based on the processed information from the hidden layers. The number of nodes in the output layer depends on the type of problem being solved (e.g., binary classification, multi-class classification, regression, etc.).\n",
    "\n",
    "4. **Weights and Biases**: Each connection between nodes in adjacent layers has an associated weight, which represents the strength of the connection. The weights determine how much influence one node has on the activation of the next. Additionally, each node in the hidden and output layers has an associated bias, which allows the neural network to model complex relationships effectively.\n",
    "\n",
    "5. **Activation Function**: Each node in the hidden and output layers uses an activation function to introduce non-linearity into the neural network. This non-linearity is crucial for the network to learn complex patterns and make accurate predictions. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, and tanh.\n",
    "\n",
    "6. **Forward Propagation**: The process of passing input data through the neural network to produce an output is called forward propagation. During this process, the input data is multiplied by the weights, and biases are added to the result. The activation function is then applied to the result, and this output becomes the input for the next layer. This process is repeated layer by layer until the output layer produces the final prediction.\n",
    "\n",
    "7. **Loss Function**: The loss function measures the difference between the predicted output and the actual target values for the given input data. The goal of training a neural network is to minimize this loss function, which represents the network's ability to make accurate predictions.\n",
    "\n",
    "8. **Backpropagation**: Backpropagation is the process of adjusting the weights and biases of the neural network to minimize the loss function. It works by computing the gradient of the loss function with respect to the network's weights and biases. The gradients are then used to update the parameters through optimization algorithms like gradient descent.\n",
    "\n",
    "9. **Training**: During the training process, the neural network is presented with a labeled dataset and iteratively adjusts its weights and biases through backpropagation to reduce the prediction errors. This is called the training process, and it continues until the network reaches satisfactory accuracy on the training data.\n",
    "\n",
    "10. **Testing and Inference**: Once the neural network is trained, it can be used for making predictions on new, unseen data. This process is called testing or inference. The neural network applies the learned patterns to the new data and produces the corresponding outputs.\n",
    "\n",
    "Neural networks have shown remarkable success in various fields, including computer vision, natural language processing, speech recognition, and more. Their ability to learn and generalize from data makes them a fundamental tool in modern artificial intelligence and machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 3s 2ms/step - loss: 0.6981 - accuracy: 0.4880\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6947 - accuracy: 0.4970\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6930 - accuracy: 0.5020\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.5340\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6907 - accuracy: 0.5330\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.5300\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6887 - accuracy: 0.5500\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6878 - accuracy: 0.5530\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6871 - accuracy: 0.5510\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6872 - accuracy: 0.5620\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "Prediction: [[0.48481032]]\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Generate some random data for demonstration\n",
    "# Replace this with your actual dataset\n",
    "data = np.random.random((1000, 20))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(20,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(data, labels, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "sample_input = np.random.random((1, 20))\n",
    "prediction = model.predict(sample_input)\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
